{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d6eeae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Marcio\n",
      "[nltk_data]     Pineda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Marcio\n",
      "[nltk_data]     Pineda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Marcio\n",
      "[nltk_data]     Pineda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m keyword_indices \u001b[38;5;241m=\u001b[39m tfidf_matrix[idx]\u001b[38;5;241m.\u001b[39mindices\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keyword_indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Verificar si la lista de índices no está vacía\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     keyword_topics \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mwhere(top_keyword_indices \u001b[38;5;241m==\u001b[39m keyword_idx)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m keyword_idx \u001b[38;5;129;01min\u001b[39;00m keyword_indices]\n\u001b[0;32m     62\u001b[0m     topic_assignments\u001b[38;5;241m.\u001b[39mappend(keyword_topics)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     59\u001b[0m keyword_indices \u001b[38;5;241m=\u001b[39m tfidf_matrix[idx]\u001b[38;5;241m.\u001b[39mindices\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keyword_indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Verificar si la lista de índices no está vacía\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     keyword_topics \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mwhere(top_keyword_indices \u001b[38;5;241m==\u001b[39m keyword_idx)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m keyword_idx \u001b[38;5;129;01min\u001b[39;00m keyword_indices]\n\u001b[0;32m     62\u001b[0m     topic_assignments\u001b[38;5;241m.\u001b[39mappend(keyword_topics)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Descargas necesarias para NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Cargar los conjuntos de datos\n",
    "ruta_train = 'C:/Users/Marcio Pineda/Documents/Archivos Python/datasets/traincase.csv'\n",
    "ruta_test = 'C:/Users/Marcio Pineda/Documents/Archivos Python/datasets/testcase.csv'\n",
    "df_train = pd.read_csv(ruta_train)\n",
    "df_test = pd.read_csv(ruta_test)\n",
    "\n",
    "# Definir funciones de preprocesamiento de texto\n",
    "def preprocess_text(text):\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Eliminación de stopwords y puntuación\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english') and token not in string.punctuation]\n",
    "    # Lematización\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocesamiento de la columna 'Keyword'\n",
    "df_train['Preprocessed Keyword'] = df_train['Keyword'].apply(preprocess_text)\n",
    "\n",
    "# Vectorización TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=600)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_train['Preprocessed Keyword'])\n",
    "\n",
    "# Modelado LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "# Obtención de los tópicos para cada keyword\n",
    "topic_keywords = lda_model.components_\n",
    "top_keyword_indices = topic_keywords.argsort(axis=1)[:, -5:]  # Obtener los índices de las 5 palabras clave principales para cada tópico\n",
    "\n",
    "# Asignación de tópicos a cada keyword\n",
    "topic_assignments = []\n",
    "for idx in range(len(df_train)):\n",
    "    keyword_indices = tfidf_matrix[idx].indices\n",
    "    if len(keyword_indices) > 0:  # Verificar si la lista de índices no está vacía\n",
    "        keyword_topics = [np.where(top_keyword_indices == keyword_idx)[0][0] for keyword_idx in keyword_indices]\n",
    "        topic_assignments.append(keyword_topics)\n",
    "    else:\n",
    "        topic_assignments.append([])  # Si no hay índices, agregar una lista vacía\n",
    "\n",
    "# Asignar los tópicos a cada keyword en el DataFrame\n",
    "df_train['Topic'] = topic_assignments\n",
    "\n",
    "def preprocess_df_general(df):\n",
    "    # Limpiar columnas numéricas, excepto 'Clicks'\n",
    "    numeric_cols = ['Search Engine Bid', 'Avg. Pos.', 'Avg. Cost per Click', 'Impressions']\n",
    "    for col in numeric_cols:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = pd.to_numeric(df[col].str.replace('$', '').str.replace(',', ''), errors='coerce')\n",
    "    df['Impressions'].fillna(df['Impressions'].median(), inplace=True)\n",
    "\n",
    "    # Procesamiento que aplica tanto al conjunto de entrenamiento como al de prueba\n",
    "    keywords_tfidf = tfidf_vectorizer.transform(df['Keyword'].str.lower())\n",
    "    keyword_clusters = kmeans.predict(keywords_tfidf)\n",
    "    df['Keyword Cluster'] = keyword_clusters\n",
    "    df['Interaction'] = df['Keyword'].astype(str) + '_' + df['Match Type'].astype(str)\n",
    "    bin_edges = [0, 100, 1000, 10000, np.inf]\n",
    "    bin_labels = [1, 2, 3, 4]\n",
    "    df['Impressions Category'] = pd.cut(df['Impressions'], bins=bin_edges, labels=bin_labels, right=False).cat.add_categories([0]).fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_df_train(df):\n",
    "    df = preprocess_df_general(df)\n",
    "    # Limpiar y convertir 'Clicks' a numérico solo para el conjunto de entrenamiento\n",
    "    df['Clicks'] = pd.to_numeric(df['Clicks'].str.replace(',', ''), errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "df_train_cleaned = preprocess_df_train(df_train.copy())\n",
    "df_test_cleaned = preprocess_df_general(df_test.copy())\n",
    "\n",
    "\n",
    "selected_features = ['Search Engine Bid', 'Impressions Category', 'Avg. Pos.', 'Keyword Cluster']\n",
    "X_train_cleaned = df_train_cleaned[selected_features]\n",
    "y_train_cleaned = df_train_cleaned['Clicks'].astype(float)\n",
    "X_train_cleaned.fillna(0, inplace=True)\n",
    "X_train_with_const_cleaned = sm.add_constant(X_train_cleaned)\n",
    "model_cleaned = sm.OLS(y_train_cleaned, X_train_with_const_cleaned)\n",
    "results_cleaned = model_cleaned.fit()\n",
    "\n",
    "print(results_cleaned.summary())\n",
    "\n",
    "# Preparar las variables para el modelo RandomForestRegressor\n",
    "X_train_with_topics = df_train_cleaned[selected_features + ['Topic']]  # Agrega la columna 'Topic' como característica\n",
    "y_train_with_topics = df_train_cleaned['Clicks']\n",
    "\n",
    "\n",
    "# Ajustar el modelo sin los tópicos\n",
    "model_rf_with_topics = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "cv_scores_with_topics = cross_val_score(model_rf_with_topics, X_train_with_topics, y_train_with_topics,\n",
    "                                        cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_with_topics = np.sqrt(-cv_scores_with_topics)\n",
    "cv_rmse_with_topics_mean = cv_rmse_with_topics.mean()\n",
    "\n",
    "print(\"RMSE promedio del modelo sin tópicos:\", cv_rmse_without_topics_mean)\n",
    "\n",
    "# Explorar Interacciones\n",
    "poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "X_interactions = poly.fit_transform(X_train_with_topics[['Search Engine Bid', 'Topic']])\n",
    "\n",
    "# Ajustar modelo con interacciones\n",
    "model_interactions = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "cv_scores_interactions = cross_val_score(model_interactions, X_interactions, y_train_with_topics,\n",
    "                                         cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_interactions = np.sqrt(-cv_scores_interactions)\n",
    "cv_rmse_interactions_mean = cv_rmse_interactions.mean()\n",
    "\n",
    "print(\"RMSE promedio del modelo con interacciones entre 'Search Engine Bid' y 'Topic':\", cv_rmse_interactions_mean)\n",
    "\n",
    "df_train_cleaned = pd.get_dummies(df_train_cleaned, columns=['Topic'])\n",
    "df_test_cleaned = pd.get_dummies(df_test_cleaned, columns=['Topic'])\n",
    "\n",
    "# Asegurarse de que ambas, train y test, tienen las mismas columnas dummy\n",
    "df_train_cleaned, df_test_cleaned = df_train_cleaned.align(df_test_cleaned, join='inner', axis=1)\n",
    "\n",
    "# Ajustar el modelo RandomForestRegressor con los tópicos ahora codificados como dummies\n",
    "model_rf_with_topics = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Asumiendo que has dividido tu df_train_cleaned en X_train_cleaned (características) y y_train_cleaned ('Clicks')\n",
    "X_train_cleaned = df_train_cleaned.drop('Clicks', axis=1)  # O cualquier otra columna que represente el objetivo\n",
    "y_train_cleaned = df_train_cleaned['Clicks']\n",
    "\n",
    "# Ahora puedes realizar cross-validation con el modelo que incluye los tópicos como características\n",
    "cv_scores_with_topics = cross_val_score(model_rf_with_topics, X_train_cleaned, y_train_cleaned, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_with_topics = np.sqrt(-cv_scores_with_topics)\n",
    "cv_rmse_with_topics_mean = cv_rmse_with_topics.mean()\n",
    "\n",
    "print(\"RMSE promedio del modelo con tópicos:\", cv_rmse_with_topics_mean)\n",
    "\n",
    "print(\"RMSE promedio del modelo con tópicos:\", cv_rmse_with_topics_mean)\n",
    "\n",
    "# Análisis de Valores Atípicos\n",
    "# Investiga los casos de valores atípicos para determinar su naturaleza\n",
    "outliers = df_train_cleaned[(np.abs(df_train_cleaned['Clicks'] - df_train_cleaned['Clicks'].mean()) > (3 * df_train_cleaned['Clicks'].std()))]\n",
    "print(\"Casos de valores atípicos:\")\n",
    "print(outliers)\n",
    "\n",
    "# Evaluación Estadística\n",
    "# Prueba de ANOVA para determinar si las diferencias en los 'Clicks' entre los tópicos son significativas\n",
    "anova_result = f_oneway(df_train_cleaned['Clicks'][df_train_cleaned['Topic'] == 'Topico1'],\n",
    "                        df_train_cleaned['Clicks'][df_train_cleaned['Topic'] == 'Topico2'],\n",
    "                        df_train_cleaned['Clicks'][df_train_cleaned['Topic'] == 'Topico3'])\n",
    "print(\"Resultados de ANOVA:\")\n",
    "print(anova_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37353a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
