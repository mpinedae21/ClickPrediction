{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf92cef",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part I: Imports and Data Check</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd71bf04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 123] The filename, directory name, or volume label syntax is incorrect: \"'C:\\\\Users\\\\Marcio Pineda\\\\Documents\\\\Archivos Python\\\\Kaggle Files (Updated)'\"\n",
      "C:\\Users\\Marcio Pineda\\Documents\\Archivos Python\\Kaggle Files (Updated)\n"
     ]
    }
   ],
   "source": [
    "%cd 'C:\\Users\\Marcio Pineda\\Documents\\Archivos Python\\Kaggle Files (Updated)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482d414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git remote set-url origin https://github.com/mpinedae21/ClickPrediction.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "git add B.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcbe6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Configurar las opciones de visualización de pandas (opcional)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Función para limpiar columnas numéricas\n",
    "def clean_numeric_column(column):\n",
    "    column_as_str = column.astype(str).str.replace(',', '').str.replace('$', '').str.strip()\n",
    "    return pd.to_numeric(column_as_str, errors='coerce')\n",
    "\n",
    "\n",
    "# Cargar los conjuntos de datos\n",
    "ruta_train = 'C:/Users/Marcio Pineda/Documents/Archivos Python/datasets/traincase.csv'\n",
    "ruta_test = 'C:/Users/Marcio Pineda/Documents/Archivos Python/datasets/testcase.csv'\n",
    "df_train = pd.read_csv(ruta_train)\n",
    "df_test = pd.read_csv(ruta_test)\n",
    "\n",
    "# Asegurarse de que 'Match Type' esté presente en los conjuntos de datos\n",
    "assert 'Match Type' in df_train.columns, \"La columna 'Match Type' no está presente en el conjunto de entrenamiento.\"\n",
    "assert 'Match Type' in df_test.columns, \"La columna 'Match Type' no está presente en el conjunto de prueba.\"\n",
    "\n",
    "# Marcar los conjuntos de datos para poder distinguirlos después de la concatenación\n",
    "df_train['set'] = 'Not Kaggle'\n",
    "df_test['set'] = 'Kaggle'\n",
    "\n",
    "# Concatenar df_train y df_test en df_full\n",
    "df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "# Aplicar la función de limpieza a las columnas numéricas relevantes en df_full\n",
    "columns_to_clean = ['Search Engine Bid', 'Impressions', 'Avg. Cost per Click', 'Avg. Pos.', 'Clicks']\n",
    "for column in columns_to_clean:\n",
    "    df_full[column] = clean_numeric_column(df_full[column])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195c108",
   "metadata": {},
   "source": [
    "<br><hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part II: Data Preparation</h2><br>\n",
    "Complete the following steps to prepare for model building. Note that you may add or remove steps as you see fit. Please see the assignment description for details on what steps are required for this project.\n",
    "<br><br>\n",
    "<h3>Feature Engineering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632f878",
   "metadata": {},
   "source": [
    "### Feature Engineering and Transformations for NLP and Keywords Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir funciones de preprocesamiento de texto\n",
    "def preprocess_text(text):\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Eliminación de stopwords y puntuación\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english') and token not in string.punctuation]\n",
    "    # Lematización\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocesamiento de la columna 'Keyword'\n",
    "df_train['Preprocessed Keyword'] = df_train['Keyword'].apply(preprocess_text)\n",
    "\n",
    "# Vectorización TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=600)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_train['Preprocessed Keyword'])\n",
    "\n",
    "# Modelado LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "# Asignar a cada muestra el tópico más probable de LDA\n",
    "df_train['Topic'] = lda_model.transform(tfidf_matrix).argmax(axis=1)\n",
    "\n",
    "# Convertir la columna 'Topic' en variables dummy\n",
    "df_train = pd.get_dummies(df_train, columns=['Topic'], drop_first=True)\n",
    "\n",
    "# Definir kmeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "# Ajustar el modelo KMeans\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "def preprocess_df_general(df, kmeans):\n",
    "    # Limpiar columnas numéricas, excepto 'Clicks'\n",
    "    numeric_cols = ['Search Engine Bid', 'Avg. Pos.', 'Avg. Cost per Click', 'Impressions']\n",
    "    for col in numeric_cols:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = pd.to_numeric(df[col].str.replace('$', '').str.replace(',', ''), errors='coerce')\n",
    "    df['Impressions'].fillna(df['Impressions'].median(), inplace=True)\n",
    "\n",
    "    # Procesamiento que aplica tanto al conjunto de entrenamiento como al de prueba\n",
    "    keywords_tfidf = tfidf_vectorizer.transform(df['Keyword'].str.lower())\n",
    "    keyword_clusters = kmeans.predict(keywords_tfidf)\n",
    "    df['Keyword Cluster'] = keyword_clusters\n",
    "    df['Interaction'] = df['Keyword'].astype(str) + '_' + df['Match Type'].astype(str)\n",
    "    bin_edges = [0, 100, 1000, 10000, np.inf]\n",
    "    bin_labels = [1, 2, 3, 4]\n",
    "    df['Impressions Category'] = pd.cut(df['Impressions'], bins=bin_edges, labels=bin_labels, right=False).cat.add_categories([0]).fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_df_train(df, kmeans):\n",
    "    df = preprocess_df_general(df, kmeans)\n",
    "    # Limpiar y convertir 'Clicks' a numérico solo para el conjunto de entrenamiento\n",
    "    df['Clicks'] = pd.to_numeric(df['Clicks'].str.replace(',', ''), errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "df_train_cleaned = preprocess_df_train(df_train.copy(), kmeans)\n",
    "df_test_cleaned = preprocess_df_general(df_test.copy(), kmeans)\n",
    "\n",
    "# Transformaciones logarítmicas\n",
    "df_train_cleaned['Log_Impressions'] = np.log1p(df_train_cleaned['Impressions'])\n",
    "# También puedes hacer lo mismo para df_test_cleaned si es necesario\n",
    "\n",
    "# Características polinómicas\n",
    "df_train_cleaned['Search_Engine_Bid_Squared'] = df_train_cleaned['Search Engine Bid'] ** 2\n",
    "df_train_cleaned['Impressions_Cubed'] = df_train_cleaned['Impressions'] ** 3\n",
    "# También puedes crear más características polinómicas según sea necesario\n",
    "\n",
    "# Actualizar las características seleccionadas\n",
    "selected_features = ['Search Engine Bid', 'Impressions Category', 'Avg. Pos.', 'Keyword Cluster',\n",
    "                     'Log_Impressions', 'Search_Engine_Bid_Squared', 'Impressions_Cubed']\n",
    "\n",
    "X_train_cleaned = df_train_cleaned[selected_features]\n",
    "y_train_cleaned = df_train_cleaned['Clicks'].astype(float)\n",
    "X_train_cleaned.fillna(0, inplace=True)\n",
    "\n",
    "# Ajustar el modelo de regresión lineal con las nuevas características\n",
    "model_cleaned = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_cleaned.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Realizar cross-validation con el modelo de regresión lineal actualizado\n",
    "cv_scores_cleaned = cross_val_score(model_cleaned, X_train_cleaned, y_train_cleaned, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_cleaned = np.sqrt(-cv_scores_cleaned)\n",
    "cv_rmse_cleaned_mean = cv_rmse_cleaned.mean()\n",
    "\n",
    "print(\"RMSE promedio del modelo limpio con transformaciones logarítmicas y características polinómicas:\", cv_rmse_cleaned_mean)\n",
    "\n",
    "# Análisis de Valores Atípicos\n",
    "# Investigar los casos de valores atípicos para determinar su naturaleza\n",
    "outliers = df_train_cleaned[(np.abs(df_train_cleaned['Clicks'] - df_train_cleaned['Clicks'].mean()) > (3 * df_train_cleaned['Clicks'].std()))]\n",
    "print(\"Casos de valores atípicos:\")\n",
    "print(outliers)\n",
    "\n",
    "# Evaluación Estadística\n",
    "# Prueba de ANOVA para determinar si las diferencias en los 'Clicks' entre los tópicos son significativas\n",
    "anova_result = f_oneway(\n",
    "    df_train_cleaned[df_train_cleaned['Topic_1'] == 1]['Clicks'],\n",
    "    df_train_cleaned[df_train_cleaned['Topic_2'] == 1]['Clicks'],\n",
    "    df_train_cleaned[df_train_cleaned['Topic_3'] == 1]['Clicks']\n",
    ")\n",
    "\n",
    "print(\"Resultados de ANOVA:\", anova_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1ad3e",
   "metadata": {},
   "source": [
    "In the first attempt at feature engineering, we tried to use NLP techniques to handle the diversity of categories that the keyword variable encompasses. We transformed these words and then tried to group them to use them as a keyword group variable in the independent variables. \n",
    "\n",
    "1. Text Preprocessing\n",
    "The first step involves cleaning and preprocessing the text in the 'Keyword' column. This includes:\n",
    "\n",
    "Tokenization: The text is broken down into tokens or individual words, turning it to lower case for consistency.\n",
    "Removal of stopwords and punctuation: Words that do not contribute significant meaning (stopwords) and punctuation marks are removed as they are not useful for text analysis.\n",
    "Lemmatization: Words are converted to their base or lemma form to reduce the variation of words with the same meaning.\n",
    "This preprocessing aims to reduce noise in the text data and highlight relevant keywords for analysis.\n",
    "\n",
    "2. TF-IDF Vectorization\n",
    "The preprocessed text is transformed into a numerical representation using TF-IDF (Term Frequency-Inverse Document Frequency), which helps identify the importance of words within documents relative to the dataset. This step is crucial for converting textual data into a format that machine learning models can process.\n",
    "\n",
    "3. LDA Modeling\n",
    "Latent Dirichlet Allocation (LDA) analysis is applied to identify themes or topics within the dataset. This model helps to group keywords into topics based on their semantic similarity, which can be useful for understanding the search categories and how they relate to clicks.\n",
    "\n",
    "4. Clustering with KMeans\n",
    "The KMeans algorithm is used to cluster keywords based on their TF-IDF characteristics. This step can reveal patterns in the keywords that might be associated with different levels of interaction, such as clicks.\n",
    "\n",
    "5. Preprocessing Numerical and Categorical Features\n",
    "Other features of the dataset, such as numerical and categorical data, are cleaned and processed. This includes:\n",
    "\n",
    "Cleaning numerical features: Converting to the proper numeric format, removing unnecessary symbols, and handling missing values.\n",
    "Creating new features: Generating new variables like 'Keyword Cluster' that might impact the prediction of clicks.\n",
    "6. Specific Preparation for the Training Set\n",
    "For the training set, additional cleaning is performed on the 'Clicks' column, ensuring it is in the proper numerical format for analysis and modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5f61a",
   "metadata": {},
   "source": [
    "<br><hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part III: Data Partitioning</h2><br>\n",
    "This is a very important step for your submission on Kaggle. Make sure to complete your data preparationbefore moving forward.\n",
    "<br>\n",
    "<br><h3>Separating the Kaggle Data</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7688658",
   "metadata": {},
   "source": [
    "### Featuring Engineering using transformations and polynomials on Impressions and Search Engine Bid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de las columnas numéricas\n",
    "def preprocess_numeric(df):\n",
    "    for col in ['Search Engine Bid', 'Avg. Pos.', 'Impressions']:\n",
    "        # Asegurar la correcta conversión de tipos de datos\n",
    "        df[col] = df[col].astype(str).str.replace('$', '').str.replace(',', '').str.strip().replace('', np.nan)\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_numeric(df_train)\n",
    "df_test = preprocess_numeric(df_test)\n",
    "\n",
    "# Imputar los valores faltantes después de la conversión\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "cols_to_impute = ['Impressions', 'Search Engine Bid', 'Avg. Pos.']\n",
    "\n",
    "df_train[cols_to_impute] = imputer.fit_transform(df_train[cols_to_impute])\n",
    "df_test[cols_to_impute] = imputer.transform(df_test[cols_to_impute])\n",
    "\n",
    "# Creación de características polinómicas\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(df_train[['Search Engine Bid', 'Impressions', 'Avg. Pos.']])\n",
    "X_test_poly = poly_features.transform(df_test[['Search Engine Bid', 'Impressions', 'Avg. Pos.']])\n",
    "\n",
    "# Separación de la variable objetivo\n",
    "# Separación de la variable objetivo\n",
    "y = df_train['Clicks'].str.replace(',', '').astype(float)  # Limpiar la columna 'Clicks' y convertir a float\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d5ebd",
   "metadata": {},
   "source": [
    "In the second attempt we used techniques with transformations for the most dominant categorical variables which were impressions and search engine bid so we used these transformation techniques and then created polynomial features between these plus average position to manage the relationships between these variables.\n",
    "\n",
    "Preprocessing of Numerical Columns\n",
    "A function named preprocess_numeric is defined to clean and convert specific columns to numeric types. This involves:\n",
    "\n",
    "Removing dollar signs and commas which could hinder numeric conversion.\n",
    "Change the columns to strings, strip any whitespace, and replace an empty string with NaN to allow numeric conversion.\n",
    "This coerces values that cannot be represented as numbers into NaNs, making sure all data on the affected columns is in a numeric format.\n",
    "This function is applied to both the training and testing datasets.\n",
    "Imputation of Missing Values\n",
    "After the conversion of columns to numeric types, there could be missing values left as a result of errors in the conversion process or due to originally missing data. A SimpleImputer with the strategy 'median' is used to fill in these missing values for specified columns. This method ensures that the model has a complete dataset to work with, improving its reliability and performance.\n",
    "\n",
    "The fit_transform method is applied to the training dataset. It computes the median values from the training data and fills the missing values in the training and validation set with those median values.\n",
    "\n",
    "The testing data is subsequently transformed, through the .transform method, of those calculated medians from the training data in order to keep consistencies.\n",
    "\n",
    "Creation of Polynomial Features\n",
    "Polynomial features are generated from the columns 'Search Engine Bid', 'Impressions', and 'Avg. Pos.' with the help of PolynomialFeatures from sklearn, but only for the interaction terms (no bias term) with the degree set to 2. Polynomial features include cross terms of all the variables, capturing the interaction between the variables—a linear model fails to capture the interactions and hence makes it powerful. Fit and transform the PolynomialFeatures only on the training data, then transform the test data in the same way. This way, the model has access to these features in both the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bea93439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE del Árbol de Decisión: 1699.2864795797498\n"
     ]
    }
   ],
   "source": [
    "# Realizar One-Hot Encoding para 'Match Type' y cualquier otra variable categórica necesaria\n",
    "df_full['Match Type'].fillna('Unknown', inplace=True)\n",
    "categorical_cols = ['Match Type']\n",
    "df_full = pd.get_dummies(df_full, columns=categorical_cols)\n",
    "\n",
    "# Llenar los NaNs restantes en las columnas numéricas con el método forward fill\n",
    "df_full.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Separación en características y objetivo, seguido por la división en entrenamiento y prueba\n",
    "features_columns = ['Search Engine Bid', 'Impressions', 'Avg. Cost per Click', 'Avg. Pos.'] + \\\n",
    "                   [col for col in df_full.columns if col.startswith('Match Type_')]\n",
    "features = df_full[df_full['set'] == 'Not Kaggle'][features_columns]\n",
    "target = df_full[df_full['set'] == 'Not Kaggle']['Clicks']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar un árbol de decisión\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred_tree = tree_model.predict(X_test)\n",
    "rmse_tree = sqrt(mean_squared_error(y_test, y_pred_tree))\n",
    "print(f\"RMSE del Árbol de Decisión: {rmse_tree}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f122a",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part III: Candidate Modeling</h2><br>\n",
    "Develop your candidate models below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac94c2",
   "metadata": {},
   "source": [
    "### Modeling using NLP and Clusters for Keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9617f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Modelos RandomForestRegressor\n",
    "model_rf = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)\n",
    "\n",
    "# Variables predictoras y variable objetivo\n",
    "X = df_train_cleaned[['Search_Engine_Bid_Squared', 'Impressions_Cubed' ]]\n",
    "y = df_train_cleaned['Clicks']\n",
    "\n",
    "# Validación cruzada con RandomForestRegressor\n",
    "cv_scores_rf = cross_val_score(model_rf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_rf = np.sqrt(-cv_scores_rf)\n",
    "cv_rmse_rf_mean = cv_rmse_rf.mean()\n",
    "\n",
    "print(\"RMSE promedio del modelo RandomForestRegressor:\", cv_rmse_rf_mean)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y validación\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar el modelo RandomForestRegressor con todos los datos de entrenamiento\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calcular el RMSE en el conjunto de entrenamiento\n",
    "train_predictions_rf = model_rf.predict(X_train)\n",
    "train_rmse_rf = np.sqrt(mean_squared_error(y_train, train_predictions_rf))\n",
    "print(\"RMSE en el conjunto de entrenamiento del modelo RandomForestRegressor:\", train_rmse_rf)\n",
    "\n",
    "# Calcular el RMSE en el conjunto de validación\n",
    "valid_predictions_rf = model_rf.predict(X_valid)\n",
    "valid_rmse_rf = np.sqrt(mean_squared_error(y_valid, valid_predictions_rf))\n",
    "print(\"RMSE en el conjunto de validación del modelo RandomForestRegressor:\", valid_rmse_rf)\n",
    "\n",
    "# Verificar sobreajuste comparando el RMSE en el conjunto de entrenamiento y el conjunto de validación\n",
    "if train_rmse_rf < valid_rmse_rf:\n",
    "    print(\"El modelo RandomForestRegressor podría estar sobreajustado.\") \n",
    "    \n",
    "    \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define los modelos de regresión lineal\n",
    "model_lr = LinearRegression()\n",
    "model_ridge = Ridge(alpha=1.0)  # Puedes ajustar el parámetro alpha según sea necesario\n",
    "model_lasso = Lasso(alpha=1.0)  # Puedes ajustar el parámetro alpha según sea necesario\n",
    "model_elasticnet = ElasticNet(alpha=1.0, l1_ratio=0.5)  # Puedes ajustar los parámetros alpha y l1_ratio según sea necesario\n",
    "\n",
    "# Variables predictoras y variable objetivo\n",
    "X_train = df_train_cleaned[['Search_Engine_Bid_Squared', 'Impressions_Cubed']]\n",
    "y_train = df_train_cleaned['Clicks']\n",
    "\n",
    "# Validación cruzada con modelos de regresión lineal\n",
    "cv_scores_lr = cross_val_score(model_lr, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_lr = np.sqrt(-cv_scores_lr)\n",
    "cv_rmse_lr_mean = cv_rmse_lr.mean()\n",
    "\n",
    "print(\"RMSE promedio del modelo LinearRegression:\", cv_rmse_lr_mean)\n",
    "\n",
    "# Validación cruzada con Ridge Regression\n",
    "cv_scores_ridge = cross_val_score(model_ridge, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_ridge = np.sqrt(-cv_scores_ridge)\n",
    "cv_rmse_ridge_mean = cv_rmse_ridge.mean()\n",
    "\n",
    "print(\"RMSE promedio del modelo Ridge Regression:\", cv_rmse_ridge_mean)\n",
    "\n",
    "# Validación cruzada con Lasso Regression\n",
    "cv_scores_lasso = cross_val_score(model_lasso, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_lasso = np.sqrt(-cv_scores_lasso)\n",
    "cv_rmse_lasso_mean = cv_rmse_lasso.mean()\n",
    "\n",
    "print(\"RMSE promedio del modelo Lasso Regression:\", cv_rmse_lasso_mean)\n",
    "\n",
    "# Validación cruzada con ElasticNet Regression\n",
    "cv_scores_elasticnet = cross_val_score(model_elasticnet, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_elasticnet = np.sqrt(-cv_scores_elasticnet)\n",
    "cv_rmse_elasticnet_mean = cv_rmse_elasticnet.mean()\n",
    "\n",
    "print(\"RMSE promedio del modelo ElasticNet Regression:\", cv_rmse_elasticnet_mean)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a575ea",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor Performance\n",
    "Average RMSE: 742.66\n",
    "Training RMSE: 307.87\n",
    "Validation RMSE: 861.12\n",
    "The training RMSE is way below the validation RMSE. It presents a great difference between the two with the RandomForestRegressor; it might just be that a model is performing well on its training data but has weaker generalization ability on new, previously unseen data. This large gap would indicate that the model is sensitive to some particular patterns in the training data, which are less common or different in the validation data.\n",
    "\n",
    "#### Linear Models Performance\n",
    "LinearRegression Average RMSE: 908.90\n",
    "Ridge Regression Average RMSE: 887.63\n",
    "Lasso Regression Average RMSE: 887.63\n",
    "ElasticNet Regression Average RMSE: 887.62 The linear models—Ridge, Lasso, and ElasticNet Regression—all cluster their RMSE value between 887 and 908. The average value of RMSE for RandomForestRegressor is lower than that for them, which, in relation to this group of methods, indicates a greater accuracy of prediction.\n",
    "However, such similar RMSE values by those models seemed to show that regularized regressions (Ridge, Lasso, ElasticNet) don't bring a good thing to the table compared to plain LinearRegression in the context at hand.\n",
    "\n",
    "#### Insights and Analysis\n",
    "\n",
    "Overfitting in RandomForestRegressor: Such a big difference between the training and validation RMSE in the RandomForestRegressor is a classical indication of overfit. The model learns from the training data too well, even capturing noise and the specific patterns which don't generalize well over new data. For example, overfitting by hyperparameter tuning, increasing min_samples_leaf or decreasing max_depth, more extensive cross-validation, or regularizing techniques could be applied.\n",
    "\n",
    "Generalization of Linear Models: The relatively stable RMSE values among models LinearRegression, Ridge, Lasso, and ElasticNet point to a fact that in this case, these perform equally to or slightly better than the RandomForestRegressor model in this case. Though an overall higher RMSE compared to the training RMSE of the RandomForestRegressor—it is indicative that they perhaps do not model with equal measure all the complexities and patterns in the data. Model selection: The choice between more complex models and simple linear models is now only a tradeoff between prediction accuracy and generalization. So, from now on, for practical purposes, it would be interesting to consider ensemble methods or look deeper into regularization techniques that could make the training performance-validation generalization gap smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f745ac",
   "metadata": {},
   "source": [
    "### Modeling using transformations and polynomials on Impressions and Search Engine Bid  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición y entrenamiento de modelos\n",
    "models = {\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'Ridge': Ridge(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(f'{name} CV RMSE:', np.sqrt(-cv_score.mean()))\n",
    "    \n",
    "    # Evaluación en el conjunto de validación\n",
    "    valid_preds = model.predict(X_valid)\n",
    "    valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_preds))\n",
    "    print(f'{name} Validation RMSE:', valid_rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045d5a5",
   "metadata": {},
   "source": [
    "####  Lasso and Ridge:\n",
    "Both models had significantly higher RMSEs on validation, about 2363, compared to the RMSEs obtained on cross-validation, which were approximately 1003. This large discrepancy suggests that while these models can capture some general trends in the data during training, they struggle to generalize well to unseen data. This could indicate an overfitting to the training data, despite the regularization applied to these models.\n",
    "\n",
    "####    Random Forest:\n",
    "The model showed superior performance on both cross-validation and the validation set, with RMSEs of approximately 744 and 994, respectively. The closer gap between these two values suggests a better generalization capability. The ensemble nature of Random Forest and its ability to handle complex interactions and nonlinearities between variables may contribute to its better performance compared to linear models like Lasso and Ridge.\n",
    "\n",
    "####     Gradient Boosting:\n",
    "Similar to Random Forest, Gradient Boosting demonstrated greater consistency between training and validation, with a CV RMSE of about 939 and a Validation RMSE of about 998. This model benefits from sequentially building trees that correct errors from previous trees, making it effective at minimizing errors on the training set while still maintaining good generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09d4a7",
   "metadata": {},
   "source": [
    "### General Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f025ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición y entrenamiento de modelos\n",
    "models = {\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'Ridge': Ridge(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(f'{name} CV RMSE:', np.sqrt(-cv_score.mean()))\n",
    "    \n",
    "    # Evaluación en el conjunto de validación\n",
    "    valid_preds = model.predict(X_valid)\n",
    "    valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_preds))\n",
    "    print(f'{name} Validation RMSE:', valid_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c58567",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8661732",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Model only using Impressions and Search Engine Bid transformations as most dominant variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587329b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Ajuste fino de hiperparámetros para Gradient Boosting (XGBoost)\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7, 1],\n",
    "    'colsample_bytree': [0.5, 0.7, 1]\n",
    "}\n",
    "grid_search_xgb = GridSearchCV(XGBRegressor(n_estimators=100, random_state=42), param_grid=param_grid_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train_pca, y_train)\n",
    "\n",
    "# Ajuste fino de hiperparámetros para LightGBM\n",
    "param_grid_lgbm = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'min_data_in_leaf': [20, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.5, 0.7, 1],\n",
    "    'colsample_bytree': [0.5, 0.7, 1]\n",
    "}\n",
    "grid_search_lgbm = GridSearchCV(LGBMRegressor(n_estimators=100, random_state=42), param_grid=param_grid_lgbm, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_lgbm.fit(X_train_pca, y_train)\n",
    "\n",
    "# Control del sobreajuste: Ensemble de modelos\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "best_lgbm_model = grid_search_lgbm.best_estimator_\n",
    "\n",
    "estimators = [('xgb', best_xgb_model), ('lgbm', best_lgbm_model)]\n",
    "stacked_model = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "stacked_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluación con Validación Cruzada\n",
    "cv_score_stacked = cross_val_score(stacked_model, X_train_pca, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Stacked Model CV RMSE:\", np.sqrt(-cv_score_stacked.mean()))\n",
    "\n",
    "# Predicción en el conjunto de validación\n",
    "stacked_valid_preds = stacked_model.predict(X_valid_pca)\n",
    "stacked_valid_rmse = np.sqrt(mean_squared_error(y_valid, stacked_valid_preds))\n",
    "print(\"Stacked Model Validation RMSE:\", stacked_valid_rmse)\n",
    "\n",
    "# Diferencia entre RMSE de entrenamiento y validación\n",
    "xgb_train_preds = best_xgb_model.predict(X_train_pca)\n",
    "xgb_valid_preds = best_xgb_model.predict(X_valid_pca)\n",
    "xgb_train_rmse = np.sqrt(mean_squared_error(y_train, xgb_train_preds))\n",
    "xgb_valid_rmse = np.sqrt(mean_squared_error(y_valid, xgb_valid_preds))\n",
    "print(\"XGBoost Train RMSE:\", xgb_train_rmse)\n",
    "print(\"XGBoost Validation RMSE:\", xgb_valid_rmse)\n",
    "print(\"Difference between XGBoost Train and Validation RMSE:\", abs(xgb_train_rmse - xgb_valid_rmse))\n",
    "\n",
    "lgbm_train_preds = best_lgbm_model.predict(X_train_pca)\n",
    "lgbm_valid_preds = best_lgbm_model.predict(X_valid_pca)\n",
    "lgbm_train_rmse = np.sqrt(mean_squared_error(y_train, lgbm_train_preds))\n",
    "lgbm_valid_rmse = np.sqrt(mean_squared_error(y_valid, lgbm_valid_preds))\n",
    "print(\"LightGBM Train RMSE:\", lgbm_train_rmse)\n",
    "print(\"LightGBM Validation RMSE:\", lgbm_valid_rmse)\n",
    "print(\"Difference between LightGBM Train and Validation RMSE:\", abs(lgbm_train_rmse - lgbm_valid_rmse))\n",
    "\n",
    "stacked_train_rmse = np.sqrt(-cv_score_stacked.mean())\n",
    "print(\"Difference between Stacked Model CV RMSE and Validation RMSE:\", abs(stacked_train_rmse - stacked_valid_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e094d6ab",
   "metadata": {},
   "source": [
    "####  Explanation : \n",
    "\n",
    "Hyperparameter Tuning for Gradient Boosting (XGBoost and LightGBM)\n",
    "XGBoost Tuning: A GridSearchCV is used to find the best hyperparameters for the XGBoost regressor. The grid search explores various combinations of learning rate, max depth, min child weight, subsample, and colsample_bytree over a 5-fold cross-validation. This exhaustive search ensures the selection of the best parameter set for minimizing the negative mean squared error (MSE).\n",
    "\n",
    "LightGBM Tuning: Similarly, a GridSearchCV is used for LightGBM, tuning parameters such as num leaves, min data in leaf, learning rate, subsample, and colsample_bytree. This process aims to optimize the LightGBM regressor by finding the best hyperparameters to minimize the negative MSE.\n",
    "\n",
    "Overfitting Control: Ensemble of Models\n",
    "After fine-tuning, the best models from XGBoost and LightGBM are used to create an ensemble using StackingRegressor. The stacking ensemble combines the predictions of the individual models and uses a RandomForestRegressor as the final estimator to predict the target variable. This method leverages the strengths of each base model and aims to improve overall prediction accuracy by reducing overfitting.\n",
    "Model Evaluation with Cross-Validation\n",
    "The stacked model is evaluated using cross-validation with a 5-fold split, calculating the root mean squared error (RMSE) to assess its performance. This step provides an unbiased estimation of the model's generalization error.\n",
    "Prediction on the Validation Set\n",
    "Predictions are made on the validation set using the stacked model, and the RMSE is calculated to measure the accuracy of these predictions. This step evaluates how well the model performs on unseen data.\n",
    "Difference Between Training and Validation RMSE\n",
    "The RMSE for both training and validation sets is calculated for XGBoost and LightGBM individually to assess the models' performance and detect any significant discrepancies, which could indicate overfitting.\n",
    "\n",
    "The difference between the cross-validation RMSE of the stacked model and its validation RMSE is calculated to further evaluate the model's generalization capability and control for overfitting.\n",
    "\n",
    "#### Results : \n",
    "\n",
    "Stacked Model CV RMSE: The Cross-Validation Root Mean Square Error (RMSE) for the stacked model is approximately 915.32. This value represents the model's average error in predicting the target variable across different subsets of the training data, providing an estimate of its generalization error.\n",
    "\n",
    "Stacked Model Validation RMSE: The RMSE on the validation set for the stacked model is about 951.37. This indicates the model's performance on unseen data, which is slightly worse than its cross-validation performance, suggesting a mild overfitting to the training data.\n",
    "\n",
    "XGBoost Train vs. Validation RMSE: The XGBoost model shows a training RMSE of approximately 697.14 and a validation RMSE of about 1324.41. The significant difference (627.27) between these two values indicates that the model performs well on the training data but poorly on unseen data, a clear sign of overfitting.\n",
    "\n",
    "LightGBM Train vs. Validation RMSE: The LightGBM model has a training RMSE of approximately 884.64 and a validation RMSE of 731.55. The difference (153.09) between these RMSE values is smaller compared to XGBoost, suggesting better generalization from training to unseen data.\n",
    "\n",
    "Difference between Stacked Model CV RMSE and Validation RMSE: The small difference (36.05) between the stacked model's CV RMSE and its validation RMSE suggests that the ensemble approach has helped to mitigate overfitting, offering a more robust model that generalizes better to unseen data.\n",
    "\n",
    "In summary, the stacked model shows a promising balance between training performance and generalization to unseen data, compared to the individual performances of XGBoost and LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36bcd7",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning using NLP and Clusters for Keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd87092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de hiperparámetros para explorar\n",
    "random_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Inicializar el modelo de RandomForest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Configurar la búsqueda aleatoria con validación cruzada\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=100,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Ajustar la búsqueda aleatoria\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Mejores hiperparámetros encontrados\n",
    "best_random_params = rf_random.best_params_\n",
    "print(\"Mejores hiperparámetros de la búsqueda aleatoria:\", best_random_params)\n",
    "\n",
    "# Ajustar el modelo con los mejores hiperparámetros en el conjunto de entrenamiento completo\n",
    "best_model_random = rf_random.best_estimator_\n",
    "\n",
    "# Calcular el RMSE en el conjunto de validación utilizando el mejor modelo de la búsqueda aleatoria\n",
    "valid_predictions_random = best_model_random.predict(X_valid)\n",
    "valid_rmse_random = np.sqrt(mean_squared_error(y_valid, valid_predictions_random))\n",
    "\n",
    "print(\"RMSE en el conjunto de validación con el mejor modelo de la búsqueda aleatoria:\", valid_rmse_random)\n",
    "\n",
    "# Ajustar el modelo con los mejores hiperparámetros en el conjunto de entrenamiento completo\n",
    "best_model_random.fit(df_train_cleaned[selected_features_extended], df_train_cleaned['Clicks'])\n",
    "\n",
    "# Calcular el RMSE en el conjunto de entrenamiento\n",
    "train_predictions_random = best_model_random.predict(df_train_cleaned[selected_features_extended])\n",
    "train_rmse_random = np.sqrt(mean_squared_error(df_train_cleaned['Clicks'], train_predictions_random))\n",
    "print(\"RMSE en el conjunto de entrenamiento con el mejor modelo de la búsqueda aleatoria:\", train_rmse_random)\n",
    "\n",
    "# Calcular el RMSE en el conjunto de validación utilizando el mejor modelo de la búsqueda aleatoria\n",
    "valid_predictions_random = best_model_random.predict(X_valid)\n",
    "valid_rmse_random = np.sqrt(mean_squared_error(y_valid, valid_predictions_random))\n",
    "print(\"RMSE en el conjunto de validación con el mejor modelo de la búsqueda aleatoria:\", valid_rmse_random)\n",
    "\n",
    "# Verificar sobreajuste comparando el RMSE en el conjunto de entrenamiento y el conjunto de validación\n",
    "if train_rmse_random < valid_rmse_random:\n",
    "    print(\"El modelo con los mejores hiperparámetros podría estar sobreajustado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17714fd8",
   "metadata": {},
   "source": [
    "#### Results :\n",
    "\n",
    "Validation Set RMSE: The RMSE on the validation set with the best model from the random search is 802.27, quantifying the model's prediction error on a dataset not used during training and providing insight into the model's generalization to new data.\n",
    "Training Set RMSE: The RMSE on the training set with the best model from the random search is 474.17, indicating the model's accuracy on the data it was trained on.\n",
    "Discrepancy in RMSE: The significant difference between the training RMSE (474.17) and the validation RMSE (802.27) signals a potential overfitting issue, where the model performs much better on the training data than on unseen data. However, the subsequent validation RMSE reported as 416.73 raises questions due to its unusual nature, suggesting a possible error in reporting or evaluation.\n",
    "\n",
    "Overfitting Concern\n",
    "The initial comparison points to potential overfitting, as the model performs better on the training data compared to the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68586968",
   "metadata": {},
   "source": [
    "### General Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78f54476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados mediante Randomized Search: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 9}\n",
      "RMSE del mejor modelo de Árbol de Decisión obtenido mediante Randomized Search: 1383.420307755057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Definir los rangos de los hiperparámetros para muestrear\n",
    "param_dist = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "# Inicializar RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=tree_model, param_distributions=param_dist, n_iter=100, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "# Realizar la búsqueda aleatoria\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params_random = random_search.best_params_\n",
    "print(\"Mejores hiperparámetros encontrados mediante Randomized Search:\", best_params_random)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_tree_model_random = random_search.best_estimator_\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "y_pred_best_random = best_tree_model_random.predict(X_test)\n",
    "rmse_best_random = sqrt(mean_squared_error(y_test, y_pred_best_random))\n",
    "print(f\"RMSE del mejor modelo de Árbol de Decisión obtenido mediante Randomized Search: {rmse_best_random}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24dffc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE del Árbol de Decisión después de Grid Search: 1259.3739580324384\n",
      "Mejores parámetros encontrados: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir los parámetros para buscar\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeRegressor(random_state=42), param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_tree_model = grid_search.best_estimator_\n",
    "y_pred_grid = best_tree_model.predict(X_test)\n",
    "rmse_grid = sqrt(mean_squared_error(y_test, y_pred_grid))\n",
    "print(f\"RMSE del Árbol de Decisión después de Grid Search: {rmse_grid}\")\n",
    "print(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b27a14e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE de Random Forest: 1220.4306512608287\n",
      "RMSE de Gradient Boosting: 1428.87882421522\n",
      "RMSE de Support Vector Regression: 923.0681355297471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Support Vector Regression': SVR()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"RMSE de {name}: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "335ee71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    }
   ],
   "source": [
    "# Definición y ajuste del modelo mediante GridSearchCV\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5,\n",
    "                           scoring='neg_root_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtención de los mejores hiperparámetros y el mejor modelo\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Preparación del conjunto de datos de prueba para predicciones finales\n",
    "df_test_processed = df_full[df_full['set'] == 'Kaggle'][features_columns]\n",
    "\n",
    "# Generación de predicciones finales para el conjunto de datos de prueba\n",
    "y_pred_test = best_model.predict(df_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "073d71e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE promedio de la validación cruzada: 743.7941138905937\n",
      "Desviación estándar del RMSE de la validación cruzada: 314.72947669398854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo con los mejores hiperparámetros encontrados\n",
    "best_model = RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100, random_state=42)\n",
    "\n",
    "# Realizar validación cruzada\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Calcular el RMSE promedio y la desviación estándar de los puntajes\n",
    "avg_rmse = -cv_scores.mean()\n",
    "std_rmse = cv_scores.std()\n",
    "\n",
    "print(f\"RMSE promedio de la validación cruzada: {avg_rmse}\")\n",
    "print(f\"Desviación estándar del RMSE de la validación cruzada: {std_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660aa65",
   "metadata": {},
   "source": [
    "####  Most important variables to predict clicks on models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fef55e8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Engine Bid      0.501392\n",
      "Impressions            0.418091\n",
      "Match Type_Exact       0.055221\n",
      "Avg. Pos.              0.014070\n",
      "Avg. Cost per Click    0.007077\n",
      "Match Type_Broad       0.003370\n",
      "Match Type_Advanced    0.000381\n",
      "Match Type_Standard    0.000312\n",
      "Match Type_Unknown     0.000087\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_importances = best_model.feature_importances_\n",
    "important_features = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False)\n",
    "print(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6fde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Definir el modelo de regresión\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Extiende el rango de hiperparámetros\n",
    "param_dist_extended = {\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': randint(2, 15),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'n_estimators': randint(100, 500)  # Aumentando el rango para n_estimators\n",
    "}\n",
    "\n",
    "# Inicializa RandomizedSearchCV con un número mayor de iteraciones\n",
    "random_search_extended = RandomizedSearchCV(estimator=model, param_distributions=param_dist_extended,\n",
    "                                             n_iter=200, cv=5, scoring='neg_mean_squared_error',\n",
    "                                             random_state=42, verbose=2, n_jobs=-1)\n",
    "\n",
    "random_search_extended.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "best_model_extended = random_search_extended.best_estimator_\n",
    "print(\"Mejores hiperparámetros:\", random_search_extended.best_params_) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e025b608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Árbol de Decisión - Mejores hiperparámetros: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 11}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Definir los rangos de los hiperparámetros para el modelo de árbol de decisión\n",
    "param_dist_tree = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "# Inicializar RandomizedSearchCV para el árbol de decisión\n",
    "random_search_tree = RandomizedSearchCV(estimator=DecisionTreeRegressor(random_state=42),\n",
    "                                        param_distributions=param_dist_tree, n_iter=200, cv=5,\n",
    "                                        scoring='neg_mean_squared_error', random_state=42, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Realizar la búsqueda aleatoria para el árbol de decisión\n",
    "random_search_tree.fit(X_train, y_train)\n",
    "\n",
    "# Obtener y mostrar los mejores hiperparámetros para el árbol de decisión\n",
    "print(\"Árbol de Decisión - Mejores hiperparámetros:\", random_search_tree.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "254019ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE del Árbol de Decisión con mejores hiperparámetros: 1259.396863451618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Mejores hiperparámetros obtenidos\n",
    "best_hyperparams = {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 11}\n",
    "\n",
    "# Entrenar el Árbol de Decisión con los mejores hiperparámetros\n",
    "best_decision_tree = DecisionTreeRegressor(max_depth=best_hyperparams['max_depth'],\n",
    "                                           min_samples_leaf=best_hyperparams['min_samples_leaf'],\n",
    "                                           min_samples_split=best_hyperparams['min_samples_split'],\n",
    "                                           random_state=42)\n",
    "best_decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones y calcular RMSE\n",
    "y_pred_decision_tree = best_decision_tree.predict(X_test)\n",
    "rmse_decision_tree = sqrt(mean_squared_error(y_test, y_pred_decision_tree))\n",
    "print(f\"RMSE del Árbol de Decisión con mejores hiperparámetros: {rmse_decision_tree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "207db715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE del ensamblaje con Stacking: 1489.7344840712556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Definir los estimadores base\n",
    "estimators = [\n",
    "    ('random_forest', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('gradient_boosting', GradientBoostingRegressor(random_state=42))\n",
    "]\n",
    "\n",
    "# Crear el ensamblaje con Stacking\n",
    "stacking_ensemble = StackingRegressor(estimators=estimators, final_estimator=DecisionTreeRegressor(max_depth=10, random_state=42))\n",
    "\n",
    "# Entrenar el ensamblaje con Stacking\n",
    "stacking_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo ensamblado con Stacking\n",
    "y_pred_stacking = stacking_ensemble.predict(X_test)\n",
    "rmse_stacking = sqrt(mean_squared_error(y_test, y_pred_stacking))\n",
    "print(f\"RMSE del ensamblaje con Stacking: {rmse_stacking}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e81653",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part IV: Preparing Submission File for Kaggle</h2><br>\n",
    "The code below will store the predicted values for each of the models above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f8bedc",
   "metadata": {},
   "source": [
    "### Submission for General Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5194006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del DataFrame de submission\n",
    "submission = pd.DataFrame({\n",
    "    'entry_id': df_test['entry_id'],  # Asegúrate de que df_test tenga 'entry_id'\n",
    "    'Clicks': y_pred_test\n",
    "})\n",
    "\n",
    "# Exportación del DataFrame de submission a un archivo CSV\n",
    "submission_filename = 'decision_tree_kaggle_submission_updated1.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107ce49",
   "metadata": {},
   "source": [
    "### Submission for LightBoost Model using transformations and polynomials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb05cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de las columnas numéricas en los datos de prueba\n",
    "df_test = preprocess_numeric(df_test)\n",
    "\n",
    "# Aplicar las mismas transformaciones al conjunto de datos de prueba que se aplicaron al conjunto de entrenamiento\n",
    "X_test_poly = poly_features.transform(df_test[['Search Engine Bid', 'Impressions', 'Avg. Pos.']])\n",
    "\n",
    "# Aplicar PCA al conjunto de datos de prueba\n",
    "X_test_pca = pca.transform(X_test_poly)  # Asumiendo que 'pca' ya está definido\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de datos de prueba utilizando el modelo entrenado\n",
    "y_pred_test_lgbm = best_lgbm_model.predict(X_test_pca)\n",
    "\n",
    "# Crear el DataFrame para el envío\n",
    "submission_lgbm = pd.DataFrame({\n",
    "    'entry_id': df_test['entry_id'],  # Asegúrate de que 'entry_id' está en el conjunto de prueba\n",
    "    'Clicks': y_pred_test_lgbm\n",
    "})\n",
    "\n",
    "# Exportar el DataFrame a un archivo CSV para el envío\n",
    "submission_filename_lgbm = 'lgbm_submission.csv'\n",
    "submission_lgbm.to_csv(submission_filename_lgbm, index=False)\n",
    "\n",
    "print(f\"Archivo de submission creado: {submission_filename_lgbm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ef057",
   "metadata": {},
   "source": [
    "### Submission for NLP and Clustering Keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb62506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo XGBoost con todo el conjunto de datos de entrenamiento\n",
    "best_xgb_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Aplicar las mismas transformaciones al conjunto de datos de prueba\n",
    "X_test_pca = pca.transform(X_test_poly)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de datos de prueba utilizando el modelo entrenado\n",
    "y_pred_test_xgb = best_xgb_model.predict(X_test_pca)\n",
    "\n",
    "# Crear el DataFrame para el envío\n",
    "submission_xgb = pd.DataFrame({\n",
    "    'entry_id': df_test['entry_id'],  # Asegúrate de que 'entry_id' está en el conjunto de prueba\n",
    "    'Clicks': y_pred_test_xgb\n",
    "})\n",
    "\n",
    "# Exportar el DataFrame a un archivo CSV para el envío\n",
    "submission_filename_xgb = 'xgb_submission.csv'\n",
    "submission_xgb.to_csv(submission_filename_xgb, index=False)\n",
    "\n",
    "print(f\"Archivo de submission creado: {submission_filename_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69dae7",
   "metadata": {},
   "source": [
    "### Conclusion and Recommendation  \n",
    "\n",
    "In this script we group 3 approaches that were applied during the business challenge. The original idea was to segment the keywords with NLP techniques in order to use decision tree models to predict clicks, this with the knowledge that impressions and search engine bid were the strongest characteristics used by the models in general to predict clicks making them the dominant variables.  In the second attempt, not finding a decrease of the RSME with the keywords, it was decided to transform only impressions and search engine bid to better fit this type of decision models and improved the cross-validation of models such as XG or Light Boost, however in kaggle the results obtained in the validation were not reflected.  Therefore, it was decided to take a more general approach by not performing transformations to the dominant variables and use the original test dataset to predict click with a basic Random Forest model and this was the one that worked best when exporting the file to kaggle. \n",
    "\n",
    "\n",
    "Therefore, if we could have used predictive models based on neural networks or try models such as SVM, perhaps we could have changed the result because it would have worked better to cover all the dimensionality that the keywords have, another option was to group by clusters based on campaigns or tourist destinations but by focusing resources on modeling and adjustment we did not manage to address this approach.  Perhaps with this the implication of the keyword variable would have been more significant for the models when trying to predict clicks, in real life we identified that this would be so from a marketing perspective.  Keyword strategies are a function of proper strategy planning, selection and adjustment of words to improve click-throughs as this best explains the amount of click-throughs you can get to the target segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9a1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
